{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565839cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from math import log\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e60280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createKmerSet(kmersize):\n",
    "    '''\n",
    "    write all possible kmers\n",
    "    :param kmersize: integer, 8\n",
    "    :return uniq_kmers: list of sorted unique kmers\n",
    "    '''\n",
    "    kmerSet = set()\n",
    "    nucleotides = [\"a\", \"c\", \"g\", \"t\"]    \n",
    "    kmerall = product(nucleotides, repeat=kmersize)\n",
    "    for i in kmerall:\n",
    "        kmer = ''.join(i)\n",
    "        kmerSet.add(kmer)\n",
    "    uniq_kmers = sorted(list(kmerSet))  \n",
    "    return uniq_kmers\n",
    "\n",
    "def compute_kmer_entropy(kmer):\n",
    "    '''\n",
    "    compute shannon entropy for each kmer\n",
    "    :param kmer: string\n",
    "    :return entropy: float\n",
    "    '''\n",
    "    prob = [float(kmer.count(c)) / len(kmer) for c in dict.fromkeys(list(kmer))]\n",
    "    entropy = - sum([ p * log(p) / log(2.0) for p in prob ])\n",
    "    return round(entropy, 2)\n",
    "\n",
    "def make_stopwords(kmersize):\n",
    "    '''\n",
    "    write filtered out kmers\n",
    "    :param kmersize: integer, 8\n",
    "    :return stopwords: list of sorted low-complexity kmers\n",
    "    '''\n",
    "    kmersize_filter = {5:1.3, 6:1.3, 7:1.3, 8:1.3, 9:1.3, 10:1.3}\n",
    "    limit_entropy = kmersize_filter.get(kmersize)\n",
    "    kmerSet = set()\n",
    "    nucleotides = [\"a\", \"c\", \"g\", \"t\"]    \n",
    "    kmerall = product(nucleotides, repeat=kmersize)\n",
    "    for n in kmerall:\n",
    "        kmer = ''.join(n)\n",
    "        if compute_kmer_entropy(kmer) < limit_entropy:\n",
    "            kmerSet.add(make_newtoken(kmer))\n",
    "        else:\n",
    "            continue\n",
    "    stopwords = sorted(list(kmerSet))\n",
    "    return stopwords\n",
    "\n",
    "def createNewtokenSet(kmersize):\n",
    "    '''\n",
    "    write all possible newtokens\n",
    "    :param kmersize: integer, 8\n",
    "    :return uniq_newtokens: list of sorted unique newtokens\n",
    "    ''' \n",
    "    newtokenSet = set()\n",
    "    uniq_kmers = createKmerSet(kmersize)\n",
    "    for kmer in uniq_kmers:\n",
    "        newtoken = make_newtoken(kmer)\n",
    "        newtokenSet.add(newtoken)  \n",
    "    uniq_newtokens = sorted(list(newtokenSet))\n",
    "    return uniq_newtokens      \n",
    "\n",
    "\n",
    "def make_newtoken(kmer):\n",
    "    '''\n",
    "    write a collapsed kmer and kmer reverse complementary as a newtoken\n",
    "    :param kmer: string e.g., \"AT\"\n",
    "    :return newtoken: string e.g., \"atnta\"\n",
    "    :param kmer: string e.g., \"TA\"\n",
    "    :return newtoken: string e.g., \"atnta\"\n",
    "    '''\n",
    "    kmer = str(kmer).lower()\n",
    "    newtoken = \"n\".join(sorted([kmer,kmer.translate(str.maketrans('tagc', 'atcg'))[::-1]]))\n",
    "    return newtoken\n",
    "\n",
    "def write_ngrams(sequence):\n",
    "    '''\n",
    "    write a bag of newtokens of size n\n",
    "    :param sequence: string e.g., \"ATCG\"\n",
    "    :param (intern) kmerlength e.g., 2\n",
    "    :return newtoken_string: string e.g., \"atnta\" \"gatc\" \"cgcg\" \n",
    "    '''\n",
    "    seq = str(sequence).lower()\n",
    "    finalstart = (len(seq)-kmerlength)+1\n",
    "    allkmers = [seq[start:(start+kmerlength)] for start in range(0,finalstart)]\n",
    "    tokens = [make_newtoken(kmer) for kmer in allkmers if len(kmer) == kmerlength and \"n\" not in kmer]\n",
    "    newtoken_string = \" \".join(tokens)\n",
    "    return newtoken_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28495c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "An example for G1 data in Arabidopsis\n",
    "'''\n",
    "\n",
    "# Set the parameters of the model\n",
    "kmerlength=7\n",
    "newtoken_size = 1+(kmerlength*2)\n",
    "all_tokens = createNewtokenSet(kmerlength)\n",
    "full=False\n",
    "stpwrds = make_stopwords(kmerlength)\n",
    "expected_tokens = len(all_tokens)\n",
    "\n",
    "# Import information of ChIP peaks and ATAC peaks in 5 species as train data sets.\n",
    "\n",
    "df=pd.read_table('kmer_AT_G1', sep=\"\\t\",\n",
    "              header=0)\n",
    "df_T = df[df.bound == 1].reset_index(drop=True)\n",
    "df_F = df[df.bound == 0].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Filter n groups of non-overlapping negative examples with the same number of positive examples, and build a list.\n",
    "df_F_list = []\n",
    "F_num = len(df_F)\n",
    "T_num = len(df_T)\n",
    "for i in range(F_num // T_num):\n",
    "    df_F_i = df_F.sample(T_num, random_state=888)\n",
    "    df_F_list.append(df_F_i.reset_index(drop=True))\n",
    "    # Update df_F, delete the part that has been taken\n",
    "    index_no_i = [i for i in df_F.index if i not in df_F_i.index]\n",
    "    df_F = df_F.loc[index_no_i].reset_index(drop=True)\n",
    "\n",
    "tmpvectorizer = TfidfVectorizer(min_df = 1 , max_df = 1.0, sublinear_tf=True,use_idf=True)\n",
    "X_TFIDF_ALL =  tmpvectorizer.fit_transform(all_tokens) #newtoken sequences to numeric index.\n",
    "vcblry = tmpvectorizer.get_feature_names()\n",
    "\n",
    "if full:\n",
    "    print(\"keeping all low-complexity k-mers\")\n",
    "    kmer_names = vcblry\n",
    "    feature_names = np.asarray(kmer_names) #key transformation to use the fancy index into the report\n",
    "else:\n",
    "    print(\"removing %d low-complexity k-mers\" % len(stpwrds))\n",
    "    kmer_names = [x for x in vcblry if x not in stpwrds]\n",
    "    feature_names = np.asarray(kmer_names) #key transformation to use the fancy index into the report\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 1 , max_df = 1.0, sublinear_tf=True,use_idf=True,vocabulary=kmer_names)\n",
    "\n",
    "tmpvectorizer = TfidfVectorizer(min_df=1,\n",
    "                                max_df=1.0,\n",
    "                                sublinear_tf=True,\n",
    "                                use_idf=True)\n",
    "X_TFIDF_ALL = tmpvectorizer.fit_transform(\n",
    "    all_tokens)  #newtoken sequences to numeric index.\n",
    "vcblry = tmpvectorizer.get_feature_names()\n",
    "def get_all_model(df_T=df_T, df_F_list=df_F_list):\n",
    "    k = 0\n",
    "    model_list = []\n",
    "    LR_hold_TFIDF_pred_list = []\n",
    "    Y_holdout_list = []\n",
    "    X_TFIDF_test_list = []\n",
    "    # Iteratively build a balanced dataset, build a model, train, save predictions.\n",
    "    for i in df_F_list:\n",
    "        df_balance_i = df_T.append(i).reset_index(drop=True)\n",
    "        train_i, test_i = train_test_split(df_balance_i,\n",
    "                                           test_size=0.2,\n",
    "                                           random_state=333,\n",
    "                                           shuffle=True)\n",
    "        train_i = train_i.reset_index(drop=True)\n",
    "        test_i = test_i.reset_index(drop=True)\n",
    "        train_i[\"tokens\"] = train_i[\"dna_string\"].apply(write_ngrams)\n",
    "        test_i[\"tokens\"] = test_i[\"dna_string\"].apply(write_ngrams)\n",
    "        train_tokens_i = train_i[\"tokens\"].tolist()\n",
    "        test_tokens_i = test_i[\"tokens\"].tolist()\n",
    "        train_labels_i = train_i[\"bound\"].tolist()\n",
    "        test_labels_i = test_i[\"bound\"].tolist()\n",
    "        unique_train_labels = len(list(set(train_labels_i)))\n",
    "        unique_test_labels = len(list(set(test_labels_i)))\n",
    "        Y_DEV_i = np.asarray(train_labels_i)\n",
    "        Y_holdout_i = np.asarray(test_labels_i)\n",
    "        X_TFIDF_DEV_i = vectorizer.fit_transform(train_tokens_i)\n",
    "        X_TFIDF_test_i = vectorizer.fit_transform(test_tokens_i)\n",
    "        locals()['TFIDF_LR_' + str(k)] = LogisticRegression(\n",
    "            C=1.0,\n",
    "            class_weight=None,\n",
    "            dual=False,\n",
    "            fit_intercept=True,\n",
    "            intercept_scaling=1,\n",
    "            max_iter=100,\n",
    "            multi_class='ovr',\n",
    "            n_jobs=1,\n",
    "            penalty='l2',\n",
    "            random_state=None,\n",
    "            solver='liblinear',\n",
    "            tol=0.0001,\n",
    "            verbose=0,\n",
    "            warm_start=False)\n",
    "        locals()['TFIDF_LR_' + str(k)].fit(X_TFIDF_DEV_i, Y_DEV_i)\n",
    "        model_list.append(locals()['TFIDF_LR_' + str(k)])\n",
    "        LR_hold_TFIDF_pred_list.append(\n",
    "            locals()['TFIDF_LR_' + str(k)].predict(X_TFIDF_test_i))\n",
    "        Y_holdout_list.extend(Y_holdout_i)\n",
    "        X_TFIDF_test_list.append(X_TFIDF_test_i)\n",
    "        k = k + 1\n",
    "    return model_list, LR_hold_TFIDF_pred_list, Y_holdout_list, X_TFIDF_test_list\n",
    "\n",
    "model_all, hold_TFIDF_pred_all, Y_holdout_all, X_TFIDF_test_all = get_all_model()\n",
    "\n",
    "\n",
    "\n",
    "# Export the kmer weights from the LR classifier to a sqlite3 database\n",
    "if hasattr(model_all[0], 'coef_'):\n",
    "    top = np.argsort(model_all[0].coef_[0])[-5:] #select the top 5 index\n",
    "    botton = np.argsort(model_all[0].coef_[0])[:5] #select the bottom 5 index\n",
    "#    logging.info(\"database table LR_results\")\n",
    "#    logging.info(\"top 5 positive kmers\")\n",
    "#    logging.info(\" \".join([ i.split('n')[0].upper() for i in feature_names[top] ]))\n",
    "#    logging.info(\" \".join([ i.split('n')[1].upper() for i in feature_names[top] ]))\n",
    "#    logging.info(\"top 5 negative kmers\")\n",
    "#    logging.info(\" \".join([ i.split('n')[0].upper() for i in feature_names[botton] ]))\n",
    "#    logging.info(\" \".join([ i.split('n')[1].upper() for i in feature_names[botton] ]))\n",
    "    print(\"Saving data to database table LR_results\")\n",
    "    print('*' * 80)\n",
    "    print(\"%s: %s\" % (\"pos kmers\", \" \".join([ i.split('n')[0].upper() for i in feature_names[top] ]) ))\n",
    "    print(\"%s: %s\" % (\"pos kmers\", \" \".join([ i.split('n')[1].upper() for i in feature_names[top] ]) ))\n",
    "    print() #making room\n",
    "    print(\"%s: %s\" % (\"neg kmers\", \" \".join([ i.split('n')[0] for i in feature_names[botton] ]) ))\n",
    "    print(\"%s: %s\" % (\"neg kmers\", \" \".join([ i.split('n')[1] for i in feature_names[botton] ]) ))\n",
    "    print('*' * 80)\n",
    "    print() #making room\n",
    "    LR_weights = []\n",
    "    for idx, kmer_score in enumerate(model_all[0].coef_[0]):\n",
    "        features = feature_names[idx].split('n')\n",
    "        LR_weights.append({'kmer':features[0].upper(),'revcomp':features[1].upper(),'score':kmer_score})\n",
    "    LR_weights_feature = pd.DataFrame(LR_weights)\n",
    "    with open(\"AT_G1_kmer_result.tsv\",\"w\") as f:\n",
    "        f.writelines('kmer'+'\\t'+'revcomp'+'\\t'+'score'+'\\n')\n",
    "        for n in range(LR_weights_feature.shape[0]):\n",
    "            f.writelines(LR_weights_feature.kmer[n]+'\\t'+LR_weights_feature.revcomp[n]+'\\t'+str(LR_weights_feature.score[n])+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bed3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble model\n",
    "proba_list = []\n",
    "for i in range(len(model_all)):\n",
    "    model_list = []\n",
    "    for j in range(len(model_all)):\n",
    "        model_list.extend(model_all[i].predict_proba(X_TFIDF_test_all[j])[:, 1])\n",
    "    proba_list.append(model_list)\n",
    "proba_ = np.array(proba_list).mean(axis=0)\n",
    "Y_pre = [0 if i < 0.5 else 1 for i in proba_]\n",
    "print('Ensemble model: ')\n",
    "print(metrics.classification_report(Y_holdout_all, Y_pre))\n",
    "\n",
    "# Draw ROC curve\n",
    "fpr,tpr,threshold = metrics.roc_curve(Y_holdout_all,proba_)\n",
    "roc_auc = metrics.auc(fpr,tpr)\n",
    "plt.stackplot(fpr,tpr,color= 'steelblue',alpha=0.3,edgecolor='black')\n",
    "plt.plot(fpr,tpr,color='black',lw=1)\n",
    "plt.plot([0,1],[0,1],color='red',linestyle='--')\n",
    "plt.text(0.5,0.3,'ROC curve (area = %.2f)' % roc_auc)\n",
    "plt.xlabel(\"1-Specificity\")\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.savefig('at_g1_kmer_roc.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
